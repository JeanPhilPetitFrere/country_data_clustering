{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('../country_data_clustering/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Data exploration & Data Cleaning ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing value\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the distribution\n",
    "for x in df.columns:\n",
    "    fig = px.histogram(df[x],title=x)\n",
    "    # fig.update_title(x)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the type of the column\n",
    "df.TotalCharges.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with the empty total charges\n",
    "df = df[df['TotalCharges'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the prices in bins\n",
    "df['TotalCharges'] = pd.cut(df.TotalCharges.astype('float64'), 10,labels=[1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second look at the distribution\n",
    "fig = px.histogram(df['TotalCharges'],title='Total Charges')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #deal with the empty total charges\n",
    "# df = df[df['TotalCharges'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Data preprocessing & feature engineering #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The k-Means and Mean Shift algorithms can only process numerical data. Therefore, we will transform source data that are not float values to integers to make them mathematically digestible.\n",
    "\n",
    "#type conversions\n",
    "\n",
    "df['tenure'] = df['tenure'].astype(np.float64) # -> Columns to which we want to apply aggregation functions such as sum or mean\n",
    "df['TotalCharges'] = df['TotalCharges'].astype(np.float64)\n",
    "df['SeniorCitizen'] = df['SeniorCitizen'].astype(object) # ->Columns that will not be subject to computations, apart from counting their elements, in preparation for the upcoming conversion steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the unique categorical element per column\n",
    "print(df.select_dtypes('object').nunique())\n",
    "\n",
    "# stats on the numerical variables\n",
    "df.select_dtypes(exclude='object').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['customerID'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the numerical values -> so the model doesn't get trapped by large differences between the column value\n",
    "numcols = list(df1.dtypes[df1.dtypes=='float64'].index)\n",
    "print(numcols)\n",
    "scaler = StandardScaler()\n",
    "df1[numcols] = scaler.fit_transform(df1[numcols])\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: translate category column to numerical column\n",
    "def catcode(df, col): \n",
    "    df[col + \"_n\"] = df[col].astype(\"category\").cat.codes\n",
    "    df = df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# create numerical columns from categories\n",
    "_ = [catcode(df1, col) for col in list(df1.dtypes[df1.dtypes == np.object].index)]\n",
    "print(df1.select_dtypes(exclude=\"object\").nunique())\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# principal components\n",
    "pca = PCA(n_components=12)\n",
    "res_pca = pca.fit_transform(df1)\n",
    "\n",
    "# scree plot\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color=\"blue\")\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "\n",
    "df1_pca = pd.DataFrame(res_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frTRAIN = 0.8               # % size of training dataset\n",
    "RNDN = 42                   # random state\n",
    "nK = 12                     # initial guess: clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow score plot with Yellowbrick\n",
    "def elbowplot(df, elbowmetric, model):\n",
    "    print(\"Elbow Score Plot (\" + str(elbowmetric) + \" metric):\")\n",
    "    vis = KElbowVisualizer(\n",
    "        model, \n",
    "        k=(2,nK), \n",
    "        metric=elbowmetric,\n",
    "        locate_elbow=True, \n",
    "        timings=False)\n",
    "    vis.fit(df)      \n",
    "    print(\"elbow value = optimal k:\", f'{vis.elbow_value_:.0f}', \\\n",
    "            \" | elbow score:\", f'{vis.elbow_score_:,.3f}')\n",
    "    vis.show()  \n",
    "    \n",
    "    \n",
    "    \n",
    "# call elbow plot for each of 3 alternative metrics\n",
    "    # distortion = mean sum of squared distances to center\n",
    "    # silhouette = mean ratio of intra-cluster and nearest-cluster distance\n",
    "    # calinski = ratio of within to between cluster dispersion\n",
    "\n",
    "model = KMeans(random_state=RNDN)\n",
    "_ = [elbowplot(df1, m, model) for m in tqdm([\"distortion\", \"silhouette\", \"calinski_harabasz\"])]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans: looking for the elbow - compare number of clusters by their inertia scores\n",
    "\n",
    "# run kMeans for alternative number of clusters k\n",
    "inertia_scores = [KMeans(\n",
    "                    n_clusters=k, \n",
    "                    init='k-means++', \n",
    "                    n_init=10, max_iter=100, random_state=RNDN). \\\n",
    "                    fit(df1).inertia_ \\\n",
    "                    for k in range(2,nK)]\n",
    "\n",
    "\n",
    "dict_inertia = dict(zip(range(2,nK), inertia_scores))\n",
    "print(\"inertia scores (sum of squared errors) by number of clusters:\")\n",
    "_ = [print(k, \":\", f'{v:,.0f}') for k,v in dict_inertia.items()]\n",
    "\n",
    "# scree plot: look for elbow\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.plot(range(2,nK), inertia_scores, color=\"blue\")\n",
    "plt.title(\"inertia (sum of squared errors) vs. number of clusters\")\n",
    "plt.xticks(np.arange(2,nK,1.0))\n",
    "plt.xlabel(\"number of clusters K\")\n",
    "plt.ylabel(\"inertia\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inertia scores: confirm visual clue of elbow plot\n",
    "# KneeLocator class will detect elbows if curve is convex; if concavem will detect knees\n",
    "inertia_knee_a3 = KneeLocator(\n",
    "        range(2,nK), \n",
    "        inertia_scores, \n",
    "        S=0.1, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "K_inertia_a3 = inertia_knee_a3.elbow   \n",
    "print(\"elbow at k =\", f'{K_inertia_a3:.0f} clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kMeans: silhouette score\n",
    "# initial example: silhouette score for 4 clusters\n",
    "k = 4\n",
    "model = KMeans(n_clusters=k, random_state=RNDN, verbose=0)\n",
    "clusters_assigned = model.fit_predict(df1)\n",
    "K_sil_a3 = silhouette_score(df1, clusters_assigned)\n",
    "print(\"silhouette score for\", k, \"clusters: \" f'{K_sil_a3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find maximum silhouette score for up to kN clusters\n",
    "sil_scores = [silhouette_score(\n",
    "                                df1, \n",
    "                                KMeans(n_clusters=k, random_state=RNDN). \\\n",
    "                                fit_predict(df1)) \\\n",
    "                                for k in tqdm(range(2,nK))]\n",
    "\n",
    "dict_sil = dict(zip(range(2,nK), sil_scores))\n",
    "print(\"silhouette scores:\")\n",
    "_ = [print(k, \":\", f'{v:,.3f}') for k,v in dict_sil.items()]\n",
    "K_sil_a3 = max(dict_sil, key=dict_sil.get)            # optimal clusters\n",
    "sil_opt_a3 = dict_sil[K_sil_a3]                       # optimal silhouette score\n",
    "print(\"maximum silhouette score for\", f'{K_sil_a3:.0f} clusters: ', f'{sil_opt_a3:.3f}')\n",
    "\n",
    "plt.figure(figsize=[7,5])\n",
    "plt.plot(range(2,nK), sil_scores, color=\"red\")\n",
    "plt.title(\"silhouette scores vs. number of clusters\")\n",
    "plt.xticks(np.arange(2,nK,1))\n",
    "plt.xlabel(\"number of clusters K\")\n",
    "plt.ylabel(\"silhouette score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette score plots with Yellowbrick\n",
    "dict_score = dict()\n",
    "fig, ax = plt.subplots(int(np.ceil(nK/2)-1), 2, figsize=(15,30))\n",
    "\n",
    "for i in tqdm(range(2,nK)):\n",
    "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=RNDN)\n",
    "    \n",
    "    q, mod = divmod(i, 2)\n",
    "    vis = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod], is_fitted=False)\n",
    "    vis.fit(df1)\n",
    "    vis.finalize()\n",
    "    dict_score[i] = vis.silhouette_score_\n",
    "\n",
    "\n",
    "print(\"silhouette scores for k clusters:\")\n",
    "_ = [print(k,\":\",f'{v:.3f}') for k,v in dict_score.items()]\n",
    "\n",
    "K_sil_a3 = max(dict_score, key=dict_score.get)          # optimal clusters\n",
    "sil_opt_a3 = dict_score[K_sil_a3]                       # optimal (maximal) silhouette score\n",
    "print(\"maximum silhouette score for\", f'{K_sil_a3:.0f} clusters: ', f'{sil_opt_a3:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimal number of clusters: intercluster distances\n",
    "model = KMeans(\n",
    "    n_clusters=K_sil_a3, init='k-means++', n_init=10, max_iter=100, random_state=RNDN)\n",
    "visD = InterclusterDistance(\n",
    "    model, max_size=20000, legend=False, random_state=RNDN)\n",
    "visD.fit(df1)\n",
    "visD.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intercluster distance maps: alternative numbers of clusters\n",
    "dict_score = dict()\n",
    "nK = 8\n",
    "fig, ax = plt.subplots(int(np.ceil(nK/2))-1, 2, figsize=(10,15))\n",
    "\n",
    "for i in tqdm(range(2,nK)):\n",
    "    km = KMeans(\n",
    "                n_clusters=i, \n",
    "                init='k-means++', n_init=10, max_iter=100, random_state=RNDN)\n",
    "    \n",
    "    q, mod = divmod(i, 2)\n",
    "    vis = InterclusterDistance(\n",
    "        km, ax=ax[q-1][mod], max_size=10000, legend=False, random_state=RNDN)\n",
    "    vis.fit(df1)\n",
    "    vis.finalize()\n",
    "    dict_score[i] = vis.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %split training vs test dataset\n",
    "df_train, df_test = train_test_split(df1, train_size=frTRAIN, random_state=RNDN)\n",
    "\n",
    "\n",
    "# training: generate \"Cluster\" column based on optimal number of clusters\n",
    "model = KMeans(n_clusters=5, random_state=RNDN)\n",
    "res = model.fit_predict(df_train)\n",
    "df_train.insert(0, \"Cluster\", res)     # insert cluster labels as new column\n",
    "df_train.tail()\n",
    "\n",
    "\n",
    "# training: get silhouette score\n",
    "sil_train = silhouette_score(df_train, res)\n",
    "# print(\"training: silhouette score for\", f'{K_sil_a3:.0f} clusters: {sil_train:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################  Interpretation: Cluster Profiling and Dashboard #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Clusters'] = model.fit_predict(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of clusters -> to check that there are no obvious outliers or exotic tiny clusters, nor a dominant cluster that dwarfs the others.\n",
    "df_grp = df1.groupby('Clusters').count()\n",
    "df_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clusters'] = df1['Clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Clusters'],\n",
    "            df['Churn'],\n",
    "            values=df['MonthlyCharges'],\n",
    "            aggfunc='mean',\n",
    "            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Clusters'],\n",
    "            df['StreamingTV'],\n",
    "            values=df['StreamingTV'],\n",
    "            aggfunc='count',\n",
    "            normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most frequent column values for each cluster\n",
    "df.groupby(['Clusters']).agg(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: pie charts for categorical variables\n",
    "def cluster_pies(df):\n",
    "    \n",
    "    # number of categorical variables\n",
    "    c = len(df.select_dtypes(\"object\").nunique())\n",
    "    \n",
    "    # number of clusters\n",
    "    K = df[\"Clusters\"].nunique()\n",
    "\n",
    "    for k in tqdm(range(K)):\n",
    "        dfc = df[df[\"Clusters\"]==k]\n",
    "        chrg = dfc[\"MonthlyCharges\"].median()\n",
    "        ten = dfc[\"tenure\"].median()\n",
    "        cases = dfc.shape[0]\n",
    "\n",
    "        fig = plt.figure(figsize=(50, 12))\n",
    "        fig.suptitle(\"Clusters \" + str(k) + \": \" + \\\n",
    "            f'{cases:,.0f}' + \" cases | \" + \\\n",
    "            \"median charge \" + f'{chrg:.2f}' + \\\n",
    "            \" | median tenure \" + f'{ten:.0f}')\n",
    "\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,0))\n",
    "        plt.pie(dfc[\"Contract\"].value_counts(), labels=dfc[\"Contract\"].unique())\n",
    "        plt.title(\"Contract\");\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,1))\n",
    "        plt.pie(dfc[\"gender\"].value_counts(), labels=dfc[\"gender\"].unique())\n",
    "        plt.title(\"gender\");\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,2))\n",
    "        plt.pie(dfc[\"SeniorCitizen\"].value_counts(), labels=dfc[\"SeniorCitizen\"].unique())\n",
    "        plt.title(\"SeniorCitizen\");\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,3))\n",
    "        plt.pie(dfc[\"Partner\"].value_counts(), labels=dfc[\"Partner\"].unique())\n",
    "        plt.title(\"Partner\");\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,4))\n",
    "        plt.pie(dfc[\"PhoneService\"].value_counts(), labels=dfc[\"PhoneService\"].unique())\n",
    "        plt.title(\"PhoneService\");\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,5))\n",
    "        plt.pie(dfc[\"InternetService\"].value_counts(), labels=dfc[\"InternetService\"].unique())\n",
    "        plt.title(\"InternetService\");\n",
    "\n",
    "        ax1 = plt.subplot2grid((2,c),(0,6))\n",
    "        plt.pie(dfc[\"StreamingTV\"].value_counts(), labels=dfc[\"StreamingTV\"].unique())\n",
    "        plt.title(\"StreamingTV\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profile(df):\n",
    "    dfc = df.groupby(\"Clusters\").agg({ \n",
    "        \"MonthlyCharges\": \"median\",\n",
    "        \"Contract\": lambda x: x.value_counts().index[0],\n",
    "        \"tenure\": \"median\",\n",
    "        \"gender\": lambda x: x.value_counts().index[0],\n",
    "        \"SeniorCitizen\": lambda x: x.value_counts().index[0],\n",
    "        \"Partner\": lambda x: x.value_counts().index[0],\n",
    "        \"Dependents\": lambda x: x.value_counts().index[0],\n",
    "        \"PhoneService\": lambda x: x.value_counts().index[0],\n",
    "        \"InternetService\": lambda x: x.value_counts().index[0],\n",
    "        \"StreamingTV\": lambda x: x.value_counts().index[0],\n",
    "        \"PaperlessBilling\": lambda x: x.value_counts().index[0],\n",
    "        \"PaymentMethod\": lambda x: x.value_counts().index[0],\n",
    "        \"Churn\": lambda x: x.value_counts().index[0]\n",
    "                                    })    #.sort_values(by=[\"MonthlyCharges\"], ascending=False)\n",
    "\n",
    "    cluster_pies(df)\n",
    "    return dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile(df).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('sampling_optimization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e17503abb0d1f10ad79304ac72ad2180e3d32cc81bb139bce04b1a9a0034495e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
